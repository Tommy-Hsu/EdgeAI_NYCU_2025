{"cells":[{"cell_type":"markdown","metadata":{"id":"qwB2JGbHhlZK"},"source":["# **Lab 2: Quantization**"]},{"cell_type":"markdown","metadata":{"id":"AMMnlB35hlZL"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"yL73oKhWhlZM"},"source":["In this lab, we use **PyTorch 2.6.0**. Please make sure to install the corresponding version to ensure correct results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2iggqUZehlZM"},"outputs":[],"source":["# install torch 2.6.0 with cuda 12.4\n","!pip3 install torch torchvision torchaudio\n","!pip3 install timm\n","!pip3 install matplotlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xV2O48cohlZN"},"outputs":[],"source":["import copy\n","import math\n","import random\n","from collections import OrderedDict, defaultdict\n","\n","from matplotlib import pyplot as plt\n","from matplotlib.colors import ListedColormap\n","import numpy as np\n","from tqdm.auto import tqdm\n","\n","import torch\n","from torch import nn"]},{"cell_type":"markdown","metadata":{"id":"w_OfKrlSh1ZO"},"source":["Should be 2.6.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6Uv3XVthq5H"},"outputs":[],"source":["print(torch.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HHusfpethlZN"},"outputs":[],"source":["random.seed(0)\n","np.random.seed(0)\n","torch.manual_seed(0)"]},{"cell_type":"markdown","metadata":{"id":"E7ULYUUNhlZN"},"source":["Test Functions **(DO NOT MODIFY!!)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jIYHNIuxhlZN"},"outputs":[],"source":["def test_linear_quantize(\n","    test_tensor=torch.tensor([\n","        [ 0.0523,  0.6364, -0.0968, -0.0020,  0.1940],\n","        [ 0.7500,  0.5507,  0.6188, -0.1734,  0.4677],\n","        [-0.0669,  0.3836,  0.4297,  0.6267, -0.0695],\n","        [ 0.1536, -0.0038,  0.6075,  0.6817,  0.0601],\n","        [ 0.6446, -0.2500,  0.5376, -0.2226,  0.2333]]),\n","    quantized_test_tensor=torch.tensor([\n","        [-1,  1, -1, -1,  0],\n","        [ 1,  1,  1, -2,  0],\n","        [-1,  0,  0,  1, -1],\n","        [-1, -1,  1,  1, -1],\n","        [ 1, -2,  1, -2,  0]], dtype=torch.int8),\n","    real_min=-0.25, real_max=0.75, bitwidth=2, scale=1/3, zero_point=-1):\n","    def plot_matrix(tensor, ax, title, vmin=0, vmax=1, cmap=ListedColormap(['white'])):\n","        ax.imshow(tensor.cpu().numpy(), vmin=vmin, vmax=vmax, cmap=cmap)\n","        ax.set_title(title)\n","        ax.set_yticklabels([])\n","        ax.set_xticklabels([])\n","        for i in range(tensor.shape[0]):\n","            for j in range(tensor.shape[1]):\n","                datum = tensor[i, j].item()\n","                if isinstance(datum, float):\n","                    text = ax.text(j, i, f'{datum:.2f}',\n","                                    ha=\"center\", va=\"center\", color=\"k\")\n","                else:\n","                    text = ax.text(j, i, f'{datum}',\n","                                    ha=\"center\", va=\"center\", color=\"k\")\n","    quantized_min, quantized_max = get_quantized_range(bitwidth)\n","    fig, axes = plt.subplots(1,3, figsize=(10, 32))\n","    plot_matrix(test_tensor, axes[0], 'original tensor', vmin=real_min, vmax=real_max)\n","    _quantized_test_tensor = linear_quantize(\n","        test_tensor, bitwidth=bitwidth, scale=scale, zero_point=zero_point)\n","    _reconstructed_test_tensor = scale * (_quantized_test_tensor.float() - zero_point)\n","    print('* Test linear_quantize()')\n","    print(f'    target bitwidth: {bitwidth} bits')\n","    print(f'        scale: {scale}')\n","    print(f'        zero point: {zero_point}')\n","    assert _quantized_test_tensor.equal(quantized_test_tensor)\n","    print('* Test passed.')\n","    plot_matrix(_quantized_test_tensor, axes[1], f'2-bit linear quantized tensor',\n","                vmin=quantized_min, vmax=quantized_max, cmap='tab20c')\n","    plot_matrix(_reconstructed_test_tensor, axes[2], f'reconstructed tensor',\n","                vmin=real_min, vmax=real_max, cmap='tab20c')\n","    fig.tight_layout()\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SbZb0L_shlZO"},"outputs":[],"source":["def test_quantized_fc(\n","    input=torch.tensor([\n","        [0.6118, 0.7288, 0.8511, 0.2849, 0.8427, 0.7435, 0.4014, 0.2794],\n","        [0.3676, 0.2426, 0.1612, 0.7684, 0.6038, 0.0400, 0.2240, 0.4237],\n","        [0.6565, 0.6878, 0.4670, 0.3470, 0.2281, 0.8074, 0.0178, 0.3999],\n","        [0.1863, 0.3567, 0.6104, 0.0497, 0.0577, 0.2990, 0.6687, 0.8626]]),\n","    weight=torch.tensor([\n","        [ 1.2626e-01, -1.4752e-01,  8.1910e-02,  2.4982e-01, -1.0495e-01,\n","         -1.9227e-01, -1.8550e-01, -1.5700e-01],\n","        [ 2.7624e-01, -4.3835e-01,  5.1010e-02, -1.2020e-01, -2.0344e-01,\n","          1.0202e-01, -2.0799e-01,  2.4112e-01],\n","        [-3.8216e-01, -2.8047e-01,  8.5238e-02, -4.2504e-01, -2.0952e-01,\n","          3.2018e-01, -3.3619e-01,  2.0219e-01],\n","        [ 8.9233e-02, -1.0124e-01,  1.1467e-01,  2.0091e-01,  1.1438e-01,\n","         -4.2427e-01,  1.0178e-01, -3.0941e-04],\n","        [-1.8837e-02, -2.1256e-01, -4.5285e-01,  2.0949e-01, -3.8684e-01,\n","         -1.7100e-01, -4.5331e-01, -2.0433e-01],\n","        [-2.0038e-01, -5.3757e-02,  1.8997e-01, -3.6866e-01,  5.5484e-02,\n","          1.5643e-01, -2.3538e-01,  2.1103e-01],\n","        [-2.6875e-01,  2.4984e-01, -2.3514e-01,  2.5527e-01,  2.0322e-01,\n","          3.7675e-01,  6.1563e-02,  1.7201e-01],\n","        [ 3.3541e-01, -3.3555e-01, -4.3349e-01,  4.3043e-01, -2.0498e-01,\n","         -1.8366e-01, -9.1553e-02, -4.1168e-01]]),\n","    bias=torch.tensor([ 0.1954, -0.2756,  0.3113,  0.1149,  0.4274,  0.2429, -0.1721, -0.2502]),\n","    quantized_bias=torch.tensor([ 3, -2,  3,  1,  3,  2, -2, -2], dtype=torch.int32),\n","    shifted_quantized_bias=torch.tensor([-1,  0, -3, -1, -3,  0,  2, -4], dtype=torch.int32),\n","    calc_quantized_output=torch.tensor([\n","        [ 0, -1,  0, -1, -1,  0,  1, -2],\n","        [ 0,  0, -1,  0,  0,  0,  0, -1],\n","        [ 0,  0,  0, -1,  0,  0,  0, -1],\n","        [ 0,  0,  0,  0,  0,  1, -1, -2]], dtype=torch.int8),\n","    bitwidth=2, batch_size=4, in_channels=8, out_channels=8):\n","    def plot_matrix(tensor, ax, title, vmin=0, vmax=1, cmap=ListedColormap(['white'])):\n","        ax.imshow(tensor.cpu().numpy(), vmin=vmin, vmax=vmax, cmap=cmap)\n","        ax.set_title(title)\n","        ax.set_yticklabels([])\n","        ax.set_xticklabels([])\n","        for i in range(tensor.shape[0]):\n","            for j in range(tensor.shape[1]):\n","                datum = tensor[i, j].item()\n","                if isinstance(datum, float):\n","                    text = ax.text(j, i, f'{datum:.2f}',\n","                                    ha=\"center\", va=\"center\", color=\"k\")\n","                else:\n","                    text = ax.text(j, i, f'{datum}',\n","                                    ha=\"center\", va=\"center\", color=\"k\")\n","\n","    output = torch.nn.functional.linear(input, weight, bias)\n","\n","    quantized_weight, weight_scale, weight_zero_point = \\\n","        linear_quantize_weight_per_channel(weight, bitwidth)\n","    quantized_input, input_scale, input_zero_point = \\\n","        linear_quantize_feature(input, bitwidth)\n","    _quantized_bias, bias_scale, bias_zero_point = \\\n","        linear_quantize_bias_per_output_channel(bias, weight_scale, input_scale)\n","    assert _quantized_bias.equal(_quantized_bias)\n","    _shifted_quantized_bias = \\\n","        shift_quantized_linear_bias(quantized_bias, quantized_weight, input_zero_point)\n","    assert _shifted_quantized_bias.equal(shifted_quantized_bias)\n","    quantized_output, output_scale, output_zero_point = \\\n","        linear_quantize_feature(output, bitwidth)\n","\n","    _calc_quantized_output = quantized_linear(\n","        quantized_input, quantized_weight, shifted_quantized_bias,\n","        bitwidth, bitwidth,\n","        input_zero_point, output_zero_point,\n","        input_scale, weight_scale, output_scale)\n","    assert _calc_quantized_output.equal(calc_quantized_output)\n","\n","    reconstructed_weight = weight_scale * (quantized_weight.float() - weight_zero_point)\n","    reconstructed_input = input_scale * (quantized_input.float() - input_zero_point)\n","    reconstructed_bias = bias_scale * (quantized_bias.float() - bias_zero_point)\n","    reconstructed_calc_output = output_scale * (calc_quantized_output.float() - output_zero_point)\n","\n","    fig, axes = plt.subplots(3,3, figsize=(15, 12))\n","    quantized_min, quantized_max = get_quantized_range(bitwidth)\n","    plot_matrix(weight, axes[0, 0], 'original weight', vmin=-0.5, vmax=0.5)\n","    plot_matrix(input.t(), axes[1, 0], 'original input', vmin=0, vmax=1)\n","    plot_matrix(output.t(), axes[2, 0], 'original output', vmin=-1.5, vmax=1.5)\n","    plot_matrix(quantized_weight, axes[0, 1], f'{bitwidth}-bit linear quantized weight',\n","                vmin=quantized_min, vmax=quantized_max, cmap='tab20c')\n","    plot_matrix(quantized_input.t(), axes[1, 1], f'{bitwidth}-bit linear quantized input',\n","                vmin=quantized_min, vmax=quantized_max, cmap='tab20c')\n","    plot_matrix(calc_quantized_output.t(), axes[2, 1], f'quantized output from quantized_linear()',\n","                vmin=quantized_min, vmax=quantized_max, cmap='tab20c')\n","    plot_matrix(reconstructed_weight, axes[0, 2], f'reconstructed weight',\n","                vmin=-0.5, vmax=0.5, cmap='tab20c')\n","    plot_matrix(reconstructed_input.t(), axes[1, 2], f'reconstructed input',\n","                vmin=0, vmax=1, cmap='tab20c')\n","    plot_matrix(reconstructed_calc_output.t(), axes[2, 2], f'reconstructed output',\n","                vmin=-1.5, vmax=1.5, cmap='tab20c')\n","\n","    print('* Test quantized_fc()')\n","    print(f'    target bitwidth: {bitwidth} bits')\n","    print(f'      batch size: {batch_size}')\n","    print(f'      input channels: {in_channels}')\n","    print(f'      output channels: {out_channels}')\n","    print('* Test passed.')\n","    fig.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ouAOztCOhlZO"},"outputs":[],"source":["def test_quantized_conv(\n","    input=torch.tensor([[\n","        [[0.6118, 0.7288, 0.8511, 0.2849],\n","         [0.8427, 0.7435, 0.4014, 0.2794],\n","         [0.3676, 0.2426, 0.1612, 0.7684],\n","         [0.6038, 0.0400, 0.2240, 0.4237]],\n","        [[0.6565, 0.6878, 0.4670, 0.3470],\n","         [0.2281, 0.8074, 0.0178, 0.3999],\n","         [0.1863, 0.3567, 0.6104, 0.0497],\n","         [0.0577, 0.2990, 0.6687, 0.8626]]]]),\n","    weight=torch.tensor([[[[ 1.2626e-01, -1.4752e-01],\n","                           [ 2.4982e-01, -1.0495e-01]],\n","                          [[-1.9227e-01, -1.8550e-01],\n","                           [-1.5700e-01,  2.7624e-01]]],\n","                         [[[-4.3835e-01,  5.1010e-02],\n","                           [-1.2020e-01, -2.0344e-01]],\n","                          [[ 1.0202e-01, -2.0799e-01],\n","                           [ 2.4112e-01, -3.8216e-01]]]]),\n","    bias=torch.tensor([0.1954, -0.2756]),\n","    quantized_bias=torch.tensor([3, -2], dtype=torch.int32),\n","    shifted_quantized_bias=torch.tensor([-1, -4], dtype=torch.int32),\n","    calc_quantized_output=torch.tensor([[[[ 0, -1,  0],\n","                       [ 0,  0,  0],\n","                       [ 0,  0,  0]],\n","                      [[-2, -1, -2],\n","                       [-1, -2,  0],\n","                       [-1, -1, -1]]]]),\n","    bitwidth=2, in_channels=2, out_channels=2):\n","\n","    def plot_matrix(tensor, ax, title, vmin=0, vmax=1, cmap=ListedColormap(['white'])):\n","        ax.imshow(tensor.cpu().numpy(), vmin=vmin, vmax=vmax, cmap=cmap)\n","        ax.set_title(title)\n","        ax.set_yticklabels([])\n","        ax.set_xticklabels([])\n","        for i in range(tensor.shape[0]):\n","            for j in range(tensor.shape[1]):\n","                datum = tensor[i, j].item()\n","                if isinstance(datum, float):\n","                    text = ax.text(j, i, f'{datum:.2f}',\n","                                    ha=\"center\", va=\"center\", color=\"k\")\n","                else:\n","                    text = ax.text(j, i, f'{datum}',\n","                                    ha=\"center\", va=\"center\", color=\"k\")\n","\n","    output = torch.nn.functional.conv2d(input, weight, bias)\n","\n","    quantized_weight, weight_scale, weight_zero_point = \\\n","        linear_quantize_weight_per_channel(weight, bitwidth)\n","    quantized_input, input_scale, input_zero_point = \\\n","        linear_quantize_feature(input, bitwidth)\n","    _quantized_bias, bias_scale, bias_zero_point = \\\n","        linear_quantize_bias_per_output_channel(bias, weight_scale, input_scale)\n","    assert _quantized_bias.equal(quantized_bias)\n","    _shifted_quantized_bias = \\\n","        shift_quantized_conv2d_bias(quantized_bias, quantized_weight, input_zero_point)\n","    assert _shifted_quantized_bias.equal(shifted_quantized_bias)\n","    quantized_output, output_scale, output_zero_point = \\\n","        linear_quantize_feature(output, bitwidth)\n","\n","    _calc_quantized_output = quantized_conv2d(\n","        quantized_input, quantized_weight, shifted_quantized_bias,\n","        bitwidth, bitwidth,\n","        input_zero_point, output_zero_point,\n","        input_scale, weight_scale, output_scale, 1, [0, 0, 0, 0], 1, 1)\n","\n","    assert _calc_quantized_output.equal(calc_quantized_output)\n","\n","    reconstructed_weight = weight_scale * (quantized_weight.float() - weight_zero_point)\n","    reconstructed_input = input_scale * (quantized_input.float() - input_zero_point)\n","    reconstructed_bias = bias_scale * (quantized_bias.float() - bias_zero_point)\n","    reconstructed_calc_output = output_scale * (calc_quantized_output.float() - output_zero_point)\n","\n","    fig, axes = plt.subplots(4,4, figsize=(25, 15))\n","    quantized_min, quantized_max = get_quantized_range(bitwidth)\n","\n","    for filter_id, filter_weight in enumerate(weight):\n","      for channel_id, channel_weight in enumerate(filter_weight):\n","        plot_matrix(channel_weight, axes[0, filter_id*len(weight)+channel_id], f'filter_{filter_id}-channel_{channel_id}', vmin=-0.5, vmax=0.5)\n","\n","\n","    for channel_id, channel_data in enumerate(input.squeeze()):\n","      plot_matrix(channel_data, axes[2, channel_id], f'original input channel_{channel_id}', vmin=0, vmax=1)\n","\n","\n","    for channel_id, channel_data in enumerate(output.squeeze()):\n","      plot_matrix(channel_data, axes[2, 2+channel_id], f'original output channel_{channel_id}', vmin=-1.5, vmax=1.5)\n","\n","    for qat_filter_id, qat_filter_weight in enumerate(quantized_weight):\n","      for qat_channel_id, qat_channel_weight in enumerate(qat_filter_weight):\n","        plot_matrix(qat_channel_weight, axes[1, qat_filter_id*len(quantized_weight)+qat_channel_id], f'{bitwidth}-bit linear quantized filter_{qat_filter_id}-channel_{qat_channel_id}',\n","                vmin=quantized_min, vmax=quantized_max, cmap='tab20c')\n","\n","    for qat_channel_id, qat_channel_data in enumerate(quantized_input.squeeze()):\n","      plot_matrix(qat_channel_data, axes[3, qat_channel_id], f'{bitwidth}-bit linear quantized input channel_{qat_channel_id}',\n","                vmin=quantized_min, vmax=quantized_max, cmap='tab20c')\n","\n","\n","    for qat_channel_id, qat_channel_data in enumerate(calc_quantized_output.squeeze()):\n","      plot_matrix(qat_channel_data, axes[3, 2+qat_channel_id], f'quantized output channel_{qat_channel_id} from quantized_conv2d()',\n","                vmin=quantized_min, vmax=quantized_max, cmap='tab20c')\n","\n","    print('* Test quantized_conv2d()')\n","    print(f'    target bitwidth: {bitwidth} bits')\n","    print(f'      input channels: {in_channels}')\n","    print(f'      output channels: {out_channels}')\n","    print('* Test passed.')\n","    fig.tight_layout()\n","    plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"YRfLKsmVhlZO"},"source":["# Part1: Linear Quantization (30%)"]},{"cell_type":"markdown","metadata":{"id":"99HWjsdchlZO"},"source":["In this section, we will implement and perform linear quantization.\n","\n","Linear quantization directly rounds the floating-point value into the nearest quantized integer after range truncation and scaling.\n","\n","[Linear quantization](https://arxiv.org/pdf/1712.05877.pdf) can be represented as\n","\n","$r = S(q-Z)$\n","\n","where $r$ is a floating point real number, $q$ is a *n*-bit integer, $Z$ is a *n*-bit integer, and $S$ is a floating point real number. $Z$ is quantization zero point and $S$ is quantization scaling factor. Both constant $Z$ and $S$ are quantization parameters."]},{"cell_type":"markdown","metadata":{"id":"TK_sQLm3hlZO"},"source":["## *n*-bit Integer\n","\n","A *n*-bit signed integer is usually represented in [two's complement](https://en.wikipedia.org/wiki/Two%27s_complement) notation.\n","\n","A *n*-bit signed integer can enode integers in the range $[-2^{n-1}, 2^{n-1}-1]$. For example, a 8-bit integer falls in the range [-128, 127]."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZoMheKamhlZP"},"outputs":[],"source":["def get_quantized_range(bitwidth):\n","    quantized_max = (1 << (bitwidth - 1)) - 1\n","    quantized_min = -(1 << (bitwidth - 1))\n","    return quantized_min, quantized_max"]},{"cell_type":"markdown","metadata":{"id":"jxLQSkXMhlZP"},"source":["## **Question 1** (5 pts)\n","\n","Please complete the following linear quantization function.\n","\n","**Hint**:\n","*   From $r=S(q-Z)$, we have $q = r/S + Z$.\n","*   Both $r$ and $S$ are floating numbers, and thus we cannot directly add integer $Z$ to $r/S$. Therefore $q = \\mathrm{int}(\\mathrm{round}(r/S)) + Z$.\n","*   To convert [`torch.FloatTensor`](https://pytorch.org/docs/stable/tensors.html) to [`torch.IntTensor`](https://pytorch.org/docs/stable/tensors.html), we could use [`torch.round()`](https://pytorch.org/docs/stable/generated/torch.round.html#torch.round), [`torch.Tensor.round()`](https://pytorch.org/docs/stable/generated/torch.Tensor.round.html#torch.Tensor.round), [`torch.Tensor.round_()`](https://pytorch.org/docs/stable/generated/torch.Tensor.round_) to first convert all values to floating integer, and then use [`torch.Tensor.to(torch.int8)`](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch.Tensor.to) to convert the data type from [`torch.float`](https://pytorch.org/docs/stable/tensors.html) to [`torch.int8`](https://pytorch.org/docs/stable/tensors.html).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u34jXbNPhlZP"},"outputs":[],"source":["def linear_quantize(fp_tensor, bitwidth, scale, zero_point, dtype=torch.int8) -> torch.Tensor:\n","    \"\"\"\n","    linear quantization for single fp_tensor\n","      from\n","        fp_tensor = (quantized_tensor - zero_point) * scale\n","      we have,\n","        quantized_tensor = int(round(fp_tensor / scale)) + zero_point\n","    :param tensor: [torch.(cuda.)FloatTensor] floating tensor to be quantized\n","    :param bitwidth: [int] quantization bit width\n","    :param scale: [torch.(cuda.)FloatTensor] scaling factor\n","    :param zero_point: [torch.(cuda.)IntTensor] the desired centroid of tensor values\n","    :return:\n","        [torch.(cuda.)FloatTensor] quantized tensor whose values are integers\n","    \"\"\"\n","    assert(fp_tensor.dtype == torch.float)\n","    assert(isinstance(scale, float) or\n","           (scale.dtype == torch.float and scale.dim() == fp_tensor.dim()))\n","    assert(isinstance(zero_point, int) or\n","           (zero_point.dtype == dtype and zero_point.dim() == fp_tensor.dim()))\n","\n","    ############### YOUR CODE STARTS HERE ###############\n","    # Step 1: scale the fp_tensor\n","    scaled_tensor =\n","    # Step 2: round the floating value to integer value\n","    rounded_tensor =\n","    ############### YOUR CODE ENDS HERE #################\n","\n","    rounded_tensor = rounded_tensor.to(dtype)\n","\n","    ############### YOUR CODE STARTS HERE ###############\n","    # Step 3: shift the rounded_tensor to make zero_point 0\n","    shifted_tensor =\n","    ############### YOUR CODE ENDS HERE #################\n","\n","    # Step 4: clamp the shifted_tensor to lie in bitwidth-bit range\n","    quantized_min, quantized_max = get_quantized_range(bitwidth)\n","    quantized_tensor = shifted_tensor.clamp_(quantized_min, quantized_max)\n","    return quantized_tensor"]},{"cell_type":"markdown","metadata":{"id":"jWTILOpchlZP"},"source":["Let's verify the functionality of defined linear quantization by applying the function above on a dummy tensor."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GvaNpSa1hlZP"},"outputs":[],"source":["test_linear_quantize()"]},{"cell_type":"markdown","metadata":{"id":"p3lEI2ERhlZP"},"source":["## **Question 2** (5 pts)\n","\n","Now we have to determine the scaling factor $S$ and zero point $Z$ for linear quantization.\n","\n","Recall that [linear quantization](https://arxiv.org/pdf/1712.05877.pdf) can be represented as\n","\n","$r = S(q-Z)$"]},{"cell_type":"markdown","metadata":{"id":"FvCcGIi1hlZP"},"source":["Linear quantization projects the floating point range [*fp_min*, *fp_max*] to the quantized range [*quantized_min*, *quantized_max*]. That is to say,\n","\n","> $r_{\\mathrm{max}} = S(q_{\\mathrm{max}}-Z)$\n",">\n","> $r_{\\mathrm{min}} = S(q_{\\mathrm{min}}-Z)$\n",">\n","> $S=(r_{\\mathrm{max}} - r_{\\mathrm{min}}) / (q_{\\mathrm{max}} - q_{\\mathrm{min}})$"]},{"cell_type":"markdown","metadata":{"id":"WtcSiwj0hlZP"},"source":["There are different approaches to determine the $r_{\\mathrm{min}}$ and  $r_{\\mathrm{max}}$ of a floating point tensor `fp_tensor`.\n","\n","*   The most common method is directly using the minimum and maximum value of `fp_tensor`.\n","*   Another widely used method is minimizing Kullback-Leibler-J divergence to determine the *fp_max*."]},{"cell_type":"markdown","metadata":{"id":"eSQhpw1hhlZP"},"source":["Once we determine the scaling factor $S$, we can directly use the relationship between $r_{\\mathrm{min}}$ and $q_{\\mathrm{min}}$ to calculate the zero point $Z$."]},{"cell_type":"markdown","metadata":{"id":"PnG91PiyhlZP"},"source":["> $Z = \\mathrm{int}(\\mathrm{round}(q_{\\mathrm{min}} - r_{\\mathrm{min}} / S))$"]},{"cell_type":"markdown","metadata":{"id":"J4pDgUuZhlZP"},"source":["Please complete the following function for calculating the scale $S$ and zero point $Z$ from floating point tensor $r$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qc8jCxRihlZP"},"outputs":[],"source":["def get_quantization_scale_and_zero_point(fp_tensor, bitwidth):\n","    \"\"\"\n","    get quantization scale for single tensor\n","    :param fp_tensor: [torch.(cuda.)Tensor] floating tensor to be quantized\n","    :param bitwidth: [int] quantization bit width\n","    :return:\n","        [float] scale\n","        [int] zero_point\n","    \"\"\"\n","    quantized_min, quantized_max = get_quantized_range(bitwidth)\n","    fp_max = fp_tensor.max().item()\n","    fp_min = fp_tensor.min().item()\n","\n","    ############### YOUR CODE STARTS HERE ###############\n","    # hint: one line of code for calculating scale\n","    scale =\n","    # hint: one line of code for calculating zero_point\n","    zero_point =\n","    ############### YOUR CODE ENDS HERE #################\n","\n","    # clip the zero_point to fall in [quantized_min, quantized_max]\n","    if zero_point < quantized_min:\n","        zero_point = quantized_min\n","    elif zero_point > quantized_max:\n","        zero_point = quantized_max\n","    else: # convert from float to int using round()\n","        zero_point = round(zero_point)\n","    return scale, int(zero_point)"]},{"cell_type":"markdown","metadata":{"id":"BArIygQPhlZP"},"source":["We now wrap  `linear_quantize()` and `get_quantization_scale_and_zero_point()` into one function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zsvkQXX3hlZQ"},"outputs":[],"source":["def linear_quantize_feature(fp_tensor, bitwidth):\n","    \"\"\"\n","    linear quantization for feature tensor\n","    :param fp_tensor: [torch.(cuda.)Tensor] floating feature to be quantized\n","    :param bitwidth: [int] quantization bit width\n","    :return:\n","        [torch.(cuda.)Tensor] quantized tensor\n","        [float] scale tensor\n","        [int] zero point\n","    \"\"\"\n","    scale, zero_point = get_quantization_scale_and_zero_point(fp_tensor, bitwidth)\n","    quantized_tensor = linear_quantize(fp_tensor, bitwidth, scale, zero_point)\n","    return quantized_tensor, scale, zero_point"]},{"cell_type":"markdown","metadata":{"id":"D-aZrGQ1hlZQ"},"source":["## **Question 3** (5 pts)\n","\n","We can apply linear quantization on model weight.\n","Let's first see the distribution of weight values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7lL66wB8hlZQ"},"outputs":[],"source":["def plot_weight_distribution(model, bitwidth=32):\n","    # bins = (1 << bitwidth) if bitwidth <= 8 else 256\n","    if bitwidth <= 8:\n","        qmin, qmax = get_quantized_range(bitwidth)\n","        bins = np.arange(qmin, qmax + 2)\n","        align = 'left'\n","    else:\n","        bins = 256\n","        align = 'mid'\n","    fig, axes = plt.subplots(9,6, figsize=(20, 15))\n","    axes = axes.ravel()\n","    plot_index = 0\n","    for name, param in model.named_parameters():\n","        if param.dim() > 1:\n","            ax = axes[plot_index]\n","            ax.hist(param.detach().view(-1).cpu(), bins=bins, density=True,\n","                    align=align, color = 'blue', alpha = 0.5,\n","                    edgecolor='black' if bitwidth <= 4 else None)\n","            if bitwidth <= 4:\n","                quantized_min, quantized_max = get_quantized_range(bitwidth)\n","                ax.set_xticks(np.arange(start=quantized_min, stop=quantized_max+1))\n","            ax.set_xlabel(name)\n","            ax.set_ylabel('density')\n","            plot_index += 1\n","    fig.suptitle(f'Histogram of Weights (bitwidth={bitwidth} bits)')\n","    fig.tight_layout()\n","    fig.subplots_adjust(top=0.925)\n","    plt.show()\n","\n","model = torch.load(\"mobilenetv2_0.963.pth\", map_location=\"cpu\", weights_only=False)\n","model.cuda()\n","plot_weight_distribution(model)"]},{"cell_type":"markdown","metadata":{"id":"8KGTJYmwhlZQ"},"source":["As we can see from the histograms above, the distribution of weight values are nearly symmetric about 0 (except for the classifier in this case). Therefore, we usually make zero point $Z=0$ when quantizating the weights.\n","\n","From $r = S(q-Z)$, we have\n","\n","> $r_{\\mathrm{max}} = S \\cdot q_{\\mathrm{max}}$\n","\n","and then\n","\n","> $S = r_{\\mathrm{max}} / q_{\\mathrm{max}}$\n","\n","We directly use the maximum magnitude of weight values as $r_{\\mathrm{max}}$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UuahHdKphlZQ"},"outputs":[],"source":["def get_quantization_scale_for_weight(weight, bitwidth):\n","    \"\"\"\n","    get quantization scale for single tensor of weight\n","    :param weight: [torch.(cuda.)Tensor] floating weight to be quantized\n","    :param bitwidth: [integer] quantization bit width\n","    :return:\n","        [floating scalar] scale\n","    \"\"\"\n","    # we just assume values in weight are symmetric\n","    # we also always make zero_point 0 for weight\n","    fp_max = max(weight.abs().max().item(), 5e-7)\n","    _, quantized_max = get_quantized_range(bitwidth)\n","\n","    ############### YOUR CODE STARTS HERE ###############\n","    # hint: one line of code for calculating scale\n","    scale =\n","    ############### YOUR CODE ENDS HERE #################\n","\n","    return scale"]},{"cell_type":"markdown","metadata":{"id":"PvFPJmSfhlZQ"},"source":["### Per-channel Linear Quantization\n","\n","Recall that for 2D convolution, the weight tensor is a 4-D tensor in the shape of (num_output_channels, num_input_channels, kernel_height, kernel_width).\n","\n","Intensive experiments show that using the different scaling factors $S$ and zero points $Z$ for different output channels will perform better. Therefore, we have to determine scaling factor $S$ and zero point $Z$ for the subtensor of each output channel independently."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xr6dsydWhlZQ"},"outputs":[],"source":["def linear_quantize_weight_per_channel(tensor, bitwidth):\n","    \"\"\"\n","    linear quantization for weight tensor\n","        using different scales and zero_points for different output channels\n","    :param tensor: [torch.(cuda.)Tensor] floating weight to be quantized\n","    :param bitwidth: [int] quantization bit width\n","    :return:\n","        [torch.(cuda.)Tensor] quantized tensor\n","        [torch.(cuda.)Tensor] scale tensor\n","        [int] zero point (which is always 0)\n","    \"\"\"\n","    dim_output_channels = 0\n","    num_output_channels = tensor.shape[dim_output_channels]\n","    scale = torch.zeros(num_output_channels, device=tensor.device)\n","    for oc in range(num_output_channels):\n","        _subtensor = tensor.select(dim_output_channels, oc)\n","        _scale = get_quantization_scale_for_weight(_subtensor, bitwidth)\n","        scale[oc] = _scale\n","    scale_shape = [1] * tensor.dim()\n","    scale_shape[dim_output_channels] = -1\n","    scale = scale.view(scale_shape)\n","    quantized_tensor = linear_quantize(tensor, bitwidth, scale, zero_point=0)\n","    return quantized_tensor, scale, 0"]},{"cell_type":"markdown","metadata":{"id":"ykz30SVVhlZQ"},"source":["### A Quick Peek at Linear Quantization on Weights\n","\n","Now let's have a peek on the weight distribution and model size when applying linear quantization on weights with different bitwidths."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f_gYNMLJhlZQ"},"outputs":[],"source":["@torch.no_grad()\n","def peek_linear_quantization():\n","    model = torch.load(\"mobilenetv2_0.963.pth\", map_location=\"cpu\", weights_only=False)\n","    for bitwidth in [4, 2]:\n","        for name, param in model.named_parameters():\n","            if param.dim() > 1:\n","                quantized_param, scale, zero_point = \\\n","                    linear_quantize_weight_per_channel(param, bitwidth)\n","                param.copy_(quantized_param)\n","        plot_weight_distribution(model, bitwidth)\n","        model = torch.load(\"mobilenetv2_0.963.pth\", map_location=\"cpu\", weights_only=False)\n","\n","\n","peek_linear_quantization()"]},{"cell_type":"markdown","metadata":{"id":"v_L8T2HIhlZQ"},"source":["## **Quantized Inference**"]},{"cell_type":"markdown","metadata":{"id":"wkZq1T08hlZQ"},"source":["After quantization, the inference of convolution and fully-connected layers also change.\n","\n","Recall that $r = S(q-Z)$, and we have\n","\n","> $r_{\\mathrm{input}} = S_{\\mathrm{input}}(q_{\\mathrm{input}}-Z_{\\mathrm{input}})$\n",">\n","> $r_{\\mathrm{weight}} = S_{\\mathrm{weight}}(q_{\\mathrm{weight}}-Z_{\\mathrm{weight}})$\n",">\n","> $r_{\\mathrm{bias}} = S_{\\mathrm{bias}}(q_{\\mathrm{bias}}-Z_{\\mathrm{bias}})$\n","\n","Since $Z_{\\mathrm{weight}}=0$, $r_{\\mathrm{weight}} = S_{\\mathrm{weight}}q_{\\mathrm{weight}}$.\n","\n","The floating point convolution can be written as,\n","\n","> $r_{\\mathrm{output}} = \\mathrm{CONV}[r_{\\mathrm{input}}, r_{\\mathrm{weight}}] + r_{\\mathrm{bias}}\\\\\n","\\;\\;\\;\\;\\;\\;\\;\\;= \\mathrm{CONV}[S_{\\mathrm{input}}(q_{\\mathrm{input}}-Z_{\\mathrm{input}}), S_{\\mathrm{weight}}q_{\\mathrm{weight}}] + S_{\\mathrm{bias}}(q_{\\mathrm{bias}}-Z_{\\mathrm{bias}})\\\\\n","\\;\\;\\;\\;\\;\\;\\;\\;= \\mathrm{CONV}[q_{\\mathrm{input}}-Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]\\cdot (S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}}) + S_{\\mathrm{bias}}(q_{\\mathrm{bias}}-Z_{\\mathrm{bias}})$\n","\n","To further simplify the computation, we could let\n","\n","> $Z_{\\mathrm{bias}} = 0$\n",">\n","> $S_{\\mathrm{bias}} = S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}}$\n","\n","so that\n","\n","> $r_{\\mathrm{output}} = (\\mathrm{CONV}[q_{\\mathrm{input}}-Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}})$\n","> $\\;\\;\\;\\;\\;\\;\\;\\;= (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}}S_{\\mathrm{weight}})$\n","\n","Since\n","> $r_{\\mathrm{output}} = S_{\\mathrm{output}}(q_{\\mathrm{output}}-Z_{\\mathrm{output}})$\n","\n","we have\n","> $S_{\\mathrm{output}}(q_{\\mathrm{output}}-Z_{\\mathrm{output}}) = (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} S_{\\mathrm{weight}})$\n","\n","and thus\n","> $q_{\\mathrm{output}} = (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}] + q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}}S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}$\n","\n","Since $Z_{\\mathrm{input}}$, $q_{\\mathrm{weight}}$, $q_{\\mathrm{bias}}$ are determined before inference, let\n","\n","> $Q_{\\mathrm{bias}} = q_{\\mathrm{bias}} - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]$\n","\n","we have\n","\n","> $q_{\\mathrm{output}} = (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Q_{\\mathrm{bias}}) \\cdot (S_{\\mathrm{input}}S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}$\n","\n","Similarily, for fully-connected layer, we have\n","\n","> $q_{\\mathrm{output}} = (\\mathrm{Linear}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}$\n","\n","where\n","\n","> $Q_{\\mathrm{bias}} = q_{\\mathrm{bias}} - \\mathrm{Linear}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]$"]},{"cell_type":"markdown","metadata":{"id":"UL1xP--bhlZR"},"source":["## **Question** 4 (5 pts)\n","\n","Please complete the following function for linear quantizing the bias.\n","\n","**Hint**:\n","\n","From the above deduction, we know that\n","\n","> $Z_{\\mathrm{bias}} = 0$\n",">\n","> $S_{\\mathrm{bias}} = S_{\\mathrm{input}} \\cdot S_{\\mathrm{weight}}$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rPmpCWyKhlZR"},"outputs":[],"source":["def linear_quantize_bias_per_output_channel(bias, weight_scale, input_scale):\n","    \"\"\"\n","    linear quantization for single bias tensor\n","        quantized_bias = fp_bias / bias_scale\n","    :param bias: [torch.FloatTensor] bias weight to be quantized\n","    :param weight_scale: [float or torch.FloatTensor] weight scale tensor\n","    :param input_scale: [float] input scale\n","    :return:\n","        [torch.IntTensor] quantized bias tensor\n","    \"\"\"\n","    assert(bias.dim() == 1)\n","    assert(bias.dtype == torch.float)\n","    assert(isinstance(input_scale, float))\n","    if isinstance(weight_scale, torch.Tensor):\n","        assert(weight_scale.dtype == torch.float)\n","        weight_scale = weight_scale.view(-1)\n","        assert(bias.numel() == weight_scale.numel())\n","\n","    ############### YOUR CODE STARTS HERE ###############\n","    # hint: one line of code\n","    bias_scale =\n","    ############### YOUR CODE ENDS HERE #################\n","\n","    quantized_bias = linear_quantize(bias, 32, bias_scale,\n","                        zero_point=0, dtype=torch.int32)\n","\n","    return quantized_bias, bias_scale, 0"]},{"cell_type":"markdown","metadata":{"id":"86SjtEUYhlZR"},"source":["## **Question 5** (5 pts)\n","\n","*Quantized Fully-Connected Layer*"]},{"cell_type":"markdown","metadata":{"id":"lGgYzFkdhlZR"},"source":["For quantized fully-connected layer, we first precompute $Q_{\\mathrm{bias}}$. Recall that $Q_{\\mathrm{bias}} = q_{\\mathrm{bias}} - \\mathrm{Linear}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m7-IdtrKhlZR"},"outputs":[],"source":["def shift_quantized_linear_bias(quantized_bias, quantized_weight, input_zero_point):\n","    \"\"\"\n","    shift quantized bias to incorporate input_zero_point for nn.Linear\n","        shifted_quantized_bias = quantized_bias - Linear(input_zero_point, quantized_weight)\n","    :param quantized_bias: [torch.IntTensor] quantized bias (torch.int32)\n","    :param quantized_weight: [torch.CharTensor] quantized weight (torch.int8)\n","    :param input_zero_point: [int] input zero point\n","    :return:\n","        [torch.IntTensor] shifted quantized bias tensor\n","    \"\"\"\n","    assert(quantized_bias.dtype == torch.int32)\n","    assert(isinstance(input_zero_point, int))\n","    return quantized_bias - quantized_weight.sum(1).to(torch.int32) * input_zero_point"]},{"cell_type":"markdown","metadata":{"id":"Rg6qFt4IhlZR"},"source":["Please complete the following quantized fully-connected layer inference function.\n","\n","**Hint**:\n","\n","> $q_{\\mathrm{output}} = (\\mathrm{Linear}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Q_{\\mathrm{bias}})\\cdot (S_{\\mathrm{input}} S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-W-KMwrJhlZR"},"outputs":[],"source":["def quantized_linear(input, weight, bias, feature_bitwidth, weight_bitwidth,\n","                     input_zero_point, output_zero_point,\n","                     input_scale, weight_scale, output_scale):\n","    \"\"\"\n","    quantized fully-connected layer\n","    :param input: [torch.CharTensor] quantized input (torch.int8)\n","    :param weight: [torch.CharTensor] quantized weight (torch.int8)\n","    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)\n","    :param feature_bitwidth: [int] quantization bit width of input and output\n","    :param weight_bitwidth: [int] quantization bit width of weight\n","    :param input_zero_point: [int] input zero point\n","    :param output_zero_point: [int] output zero point\n","    :param input_scale: [float] input feature scale\n","    :param weight_scale: [torch.FloatTensor] weight per-channel scale\n","    :param output_scale: [float] output feature scale\n","    :return:\n","        [torch.CharIntTensor] quantized output feature (torch.int8)\n","    \"\"\"\n","    assert(input.dtype == torch.int8)\n","    assert(weight.dtype == input.dtype)\n","    assert(bias is None or bias.dtype == torch.int32)\n","    assert(isinstance(input_zero_point, int))\n","    assert(isinstance(output_zero_point, int))\n","    assert(isinstance(input_scale, float))\n","    assert(isinstance(output_scale, float))\n","    assert(weight_scale.dtype == torch.float)\n","\n","    # Step 1: integer-based fully-connected (8-bit multiplication with 32-bit accumulation)\n","    if 'cpu' in input.device.type:\n","        # use 32-b MAC for simplicity\n","        output = torch.nn.functional.linear(input.to(torch.int32), weight.to(torch.int32), bias)\n","    else:\n","        # current version pytorch does not yet support integer-based linear() on GPUs\n","        output = torch.nn.functional.linear(input.float(), weight.float(), bias.float())\n","\n","    ############### YOUR CODE STARTS HERE ###############\n","    # Step 2: scale the output\n","    #         hint: 1. scales are floating numbers, we need to convert output to float as well\n","    #               2. the shape of weight scale is [oc, 1, 1, 1] while the shape of output is [batch_size, oc]\n","    output =\n","    # Step 3: shift output by output_zero_point\n","    #         hint: one line of code\n","    output =\n","    ############### YOUR CODE ENDS HERE #################\n","\n","    # Make sure all value lies in the bitwidth-bit range\n","    output = output.round().clamp(*get_quantized_range(feature_bitwidth)).to(torch.int8)\n","    return output"]},{"cell_type":"markdown","metadata":{"id":"xD1p1twXhlZR"},"source":["Let's verify the functionality of defined quantized fully connected layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZKp4HfAOhlZR"},"outputs":[],"source":["test_quantized_fc()"]},{"cell_type":"markdown","metadata":{"id":"cvx_IojchlZR"},"source":["## **Question 6** (5 pts)\n","\n","*Quantized Convolution Layer*"]},{"cell_type":"markdown","metadata":{"id":"utHc6oGghlZR"},"source":["For quantized convolution layer, we first precompute $Q_{\\mathrm{bias}}$. Recall that $Q_{\\mathrm{bias}} = q_{\\mathrm{bias}} - \\mathrm{CONV}[Z_{\\mathrm{input}}, q_{\\mathrm{weight}}]$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5xMeiB5fhlZR"},"outputs":[],"source":["def shift_quantized_conv2d_bias(quantized_bias, quantized_weight, input_zero_point):\n","    \"\"\"\n","    shift quantized bias to incorporate input_zero_point for nn.Conv2d\n","        shifted_quantized_bias = quantized_bias - Conv(input_zero_point, quantized_weight)\n","    :param quantized_bias: [torch.IntTensor] quantized bias (torch.int32)\n","    :param quantized_weight: [torch.CharTensor] quantized weight (torch.int8)\n","    :param input_zero_point: [int] input zero point\n","    :return:\n","        [torch.IntTensor] shifted quantized bias tensor\n","    \"\"\"\n","    assert(quantized_bias.dtype == torch.int32)\n","    assert(isinstance(input_zero_point, int))\n","    return quantized_bias - quantized_weight.sum((1,2,3)).to(torch.int32) * input_zero_point"]},{"cell_type":"markdown","metadata":{"id":"R9KVkQHshlZS"},"source":["Please complete the following quantized convolution function.\n","\n","**Hint**:\n","> $q_{\\mathrm{output}} = (\\mathrm{CONV}[q_{\\mathrm{input}}, q_{\\mathrm{weight}}] + Q_{\\mathrm{bias}}) \\cdot (S_{\\mathrm{input}}S_{\\mathrm{weight}} / S_{\\mathrm{output}}) + Z_{\\mathrm{output}}$\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mjisd7FihlZS"},"outputs":[],"source":["def quantized_conv2d(input, weight, bias, feature_bitwidth, weight_bitwidth,\n","                     input_zero_point, output_zero_point,\n","                     input_scale, weight_scale, output_scale,\n","                     stride, padding, dilation, groups):\n","    \"\"\"\n","    quantized 2d convolution\n","    :param input: [torch.CharTensor] quantized input (torch.int8)\n","    :param weight: [torch.CharTensor] quantized weight (torch.int8)\n","    :param bias: [torch.IntTensor] shifted quantized bias or None (torch.int32)\n","    :param feature_bitwidth: [int] quantization bit width of input and output\n","    :param weight_bitwidth: [int] quantization bit width of weight\n","    :param input_zero_point: [int] input zero point\n","    :param output_zero_point: [int] output zero point\n","    :param input_scale: [float] input feature scale\n","    :param weight_scale: [torch.FloatTensor] weight per-channel scale\n","    :param output_scale: [float] output feature scale\n","    :return:\n","        [torch.(cuda.)CharTensor] quantized output feature\n","    \"\"\"\n","    assert(len(padding) == 4)\n","    assert(input.dtype == torch.int8)\n","    assert(weight.dtype == input.dtype)\n","    assert(bias is None or bias.dtype == torch.int32)\n","    assert(isinstance(input_zero_point, int))\n","    assert(isinstance(output_zero_point, int))\n","    assert(isinstance(input_scale, float))\n","    assert(isinstance(output_scale, float))\n","    assert(weight_scale.dtype == torch.float)\n","\n","    # Step 1: calculate integer-based 2d convolution (8-bit multiplication with 32-bit accumulation)\n","    input = torch.nn.functional.pad(input, padding, 'constant', input_zero_point)\n","    if 'cpu' in input.device.type:\n","        # use 32-b MAC for simplicity\n","        output = torch.nn.functional.conv2d(input.to(torch.int32), weight.to(torch.int32), None, stride, 0, dilation, groups)\n","    else:\n","        # current version pytorch does not yet support integer-based conv2d() on GPUs\n","        output = torch.nn.functional.conv2d(input.float(), weight.float(), None, stride, 0, dilation, groups)\n","        output = output.round().to(torch.int32)\n","    if bias is not None:\n","        output = output + bias.view(1, -1, 1, 1)\n","\n","    ############### YOUR CODE STARTS HERE ###############\n","    # hint: this code block should be the very similar to quantized_linear()\n","\n","    # Step 2: scale the output\n","    #         hint: 1. scales are floating numbers, we need to convert output to float as well\n","    #               2. the shape of weight scale is [oc, 1, 1, 1] while the shape of output is [batch_size, oc, height, width]\n","    output =\n","    # Step 3: shift output by output_zero_point\n","    #         hint: one line of code\n","    output =\n","    ############### YOUR CODE ENDS HERE #################\n","\n","    # Make sure all value lies in the bitwidth-bit range\n","    output = output.round().clamp(*get_quantized_range(feature_bitwidth)).to(torch.int8)\n","    return output"]},{"cell_type":"markdown","metadata":{"id":"K8003QWlhlZS"},"source":["Let's verify the functionality of defined quantized convolution layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MgON3Yf_hlZS"},"outputs":[],"source":["test_quantized_conv()"]},{"cell_type":"markdown","metadata":{"id":"2PAW45yCMAM9"},"source":["# Part2: Quantize DeiT-S & SLM (70%)"]},{"cell_type":"markdown","metadata":{"id":"NklNz7CcMAM9"},"source":["In this section, you are required to quantize the `Llama3.1-1B-Instruct` model.\n","\n","Please refer to `run_slm.py`"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}