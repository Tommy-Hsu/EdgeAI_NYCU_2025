{"cells":[{"cell_type":"markdown","metadata":{"id":"PFbUpH8elWQ7"},"source":["# **Part 1: Run MobileNet on GPU**"]},{"cell_type":"markdown","metadata":{"id":"aoNr0MWd5e5m"},"source":["In this tutorial, we will explore how to train a neural network with PyTorch."]},{"cell_type":"markdown","metadata":{"id":"yoBtxdvR5lwM"},"source":["### Setup (5%)"]},{"cell_type":"markdown","metadata":{"id":"0oLGv2RjLYh2"},"source":["We will first install a few packages that will be used in this tutorial and also define the path of CUDA library:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3r7Sl2cG7nZF"},"outputs":[],"source":["!pip install torchprofile 1>/dev/null\n","!ldconfig /usr/lib64-nvidia 2>/dev/null\n","!pip install onnx 1>/dev/null\n","!pip install onnxruntime 1>/dev/null"]},{"cell_type":"markdown","metadata":{"id":"LYgp0au_LeAd"},"source":["We will then import a few libraries:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I3uAhaCSlFrK"},"outputs":[],"source":["import random\n","\n","import numpy as np\n","import torch\n","import torchvision\n","from torch import nn\n","from torch.optim import *\n","from torch.optim.lr_scheduler import *\n","from torch.utils.data import DataLoader\n","from torchprofile import profile_macs\n","from torchvision.datasets import *\n","from torchvision.transforms import *\n","from tqdm.auto import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D-nNU83gqm9U"},"outputs":[],"source":["print(torch.__version__)\n","print(torchvision.__version__)"]},{"cell_type":"markdown","metadata":{"id":"u1Yx0rDUK5fx"},"source":["To ensure the reproducibility, we will control the seed of random generators:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j_l1wEdeHOlu"},"outputs":[],"source":["random.seed(0)\n","np.random.seed(0)\n","torch.manual_seed(0)"]},{"cell_type":"markdown","metadata":{"id":"JzMCWN0aJNvl"},"source":["We must decide the HYPER-parameter before training the model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AyLHUYWZJNCJ"},"outputs":[],"source":["NUM_CLASSES = 10\n","\n","# TODO:\n","# Decide your own hyper-parameters\n","BATCH_SIZE =\n","LEARNING_RATE =\n","NUM_EPOCH ="]},{"cell_type":"markdown","metadata":{"id":"u7Y0sLyajGAu"},"source":["### Data  (5%)"]},{"cell_type":"markdown","metadata":{"id":"VAbL_li0KPsz"},"source":["In this lab, we will use CIFAR-10 as our target dataset. This dataset contains images from 10 classes, where each image is of\n","size 3x32x32, i.e. 3-channel color images of 32x32 pixels in size."]},{"cell_type":"markdown","metadata":{"id":"7PO2mP2GytNl"},"source":["Before using the data as input, we can do data pre-processing with transform function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pqhy8EJSjJfp"},"outputs":[],"source":["# TODO:\n","# Resize images to 224x224, i.e., the input image size of MobileNet,\n","# Convert images to PyTorch tensors, and\n","# Normalize the images with mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n","transform =\n","\n","\n","dataset = {}\n","for split in [\"train\", \"test\"]:\n","  dataset[split] = CIFAR10(\n","    root=\"data/cifar10\",\n","    train=(split == \"train\"),\n","    download=True,\n","    transform=transform,\n","  )"]},{"cell_type":"markdown","metadata":{"id":"jkigVqADNeIN"},"source":["To train a neural network, we will need to feed data in batches.\n","\n","We create data loaders with the batch size determined previously in setup section:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4axnQCtnks_s"},"outputs":[],"source":["dataflow = {}\n","for split in ['train', 'test']:\n","  dataflow[split] = DataLoader(\n","    dataset[split],\n","    batch_size=BATCH_SIZE,\n","    shuffle=(split == 'train'),\n","    num_workers=0,\n","    pin_memory=True,\n","    drop_last=True\n","  )"]},{"cell_type":"markdown","metadata":{"id":"_5G1Lf6hOLGT"},"source":["We can print the data type and shape from the training data loader:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ReP2g9pD6ppI"},"outputs":[],"source":["for inputs, targets in dataflow[\"train\"]:\n","  print(f\"[inputs] dtype: {inputs.dtype}, shape: {inputs.shape}\")\n","  print(f\"[targets] dtype: {targets.dtype}, shape: {targets.shape}\")\n","  break"]},{"cell_type":"markdown","metadata":{"id":"sPAEVnixjwb7"},"source":["### Model (10%)"]},{"cell_type":"markdown","metadata":{"id":"rFr1Js3-e3rJ"},"source":["In this tutorial, we will import MobileNet provided by torchvision, and use the pre-trained weight:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SNLdS_UQjyBf"},"outputs":[],"source":["# TODO:\n","# Load pre-trained MobileNetV2\n","from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n","model =\n","print(model)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WB2X6czdV0px"},"source":["You should observe that the output dimension of the classifier does not match the number of cleasses in CIFAR-10.\n","\n","Now change the output dimension of the classifer to number of classes:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nmo4u51gVzXz"},"outputs":[],"source":["# TODO:\n","# Change the output dimension of the classifer to number of classes\n","model.classifier[1] =\n","print(model)\n","\n","# Send the model from cpu to gpu\n","model ="]},{"cell_type":"markdown","metadata":{"id":"WEtm9nswWT1z"},"source":["Now the output dimension of the classifer matches."]},{"cell_type":"markdown","metadata":{"id":"F_RcCWoQ8Kp1"},"source":["As this course focuses on efficiency, we will then inspect its model size and (theoretical) computation cost.\n"]},{"cell_type":"markdown","metadata":{"id":"Zd4Xu-vMyz39"},"source":["* The model size can be estimated by the number of trainable parameters:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4gTfqC0B7Uzi"},"outputs":[],"source":["num_params = 0\n","for param in model.parameters():\n","  if param.requires_grad:\n","    num_params += param.numel()\n","print(\"#Params:\", num_params)"]},{"cell_type":"markdown","metadata":{"id":"uAZoIKIbzLa4"},"source":["* The computation cost can be estimated by the number of [multiply–accumulate operations (MACs)](https://en.wikipedia.org/wiki/Multiply–accumulate_operation) using [TorchProfile](https://github.com/zhijian-liu/torchprofile), we will further use this profiling tool in the future labs ."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OKVmyWCN7qpp"},"outputs":[],"source":["num_macs = profile_macs(model, torch.zeros(1, 3, 224, 224).cuda())\n","print(\"#MACs:\", num_macs)"]},{"cell_type":"markdown","metadata":{"id":"OYkqpfejzxwq"},"source":["This model has 2.2M parameters and requires 306M MACs for inference. We will work together in the next few labs to improve its efficiency."]},{"cell_type":"markdown","metadata":{"id":"gjDsY9_KkIjZ"},"source":["### Optimization (10%)"]},{"cell_type":"markdown","metadata":{"id":"oRg_5KeKLHPj"},"source":["As we are working on a classification problem, we will apply [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) as our loss function to optimize the model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-K0DEhGKkKfF"},"outputs":[],"source":["# TODO:\n","# Apply cross entropy as our loss function\n","criterion ="]},{"cell_type":"markdown","metadata":{"id":"3H8YniYeLIdg"},"source":["We should decide an optimizer for the model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HXANib83LATH"},"outputs":[],"source":["# TODO:\n","# Choose an optimizer.\n","optimizer ="]},{"cell_type":"markdown","metadata":{"id":"v9X8SiWYLJw2"},"source":["(Optional) We can apply a learning rate scheduler during the training:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8mJU5aw8KrVX"},"outputs":[],"source":["# TODO(optional):\n","scheduler ="]},{"cell_type":"markdown","metadata":{"id":"i2UFRbRYly50"},"source":["### Training (25%)"]},{"cell_type":"markdown","metadata":{"id":"IpHZJpjR7Wy3"},"source":["We first define the function that optimizes the model for one batch:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"79GKx_oVl09b"},"outputs":[],"source":["def train_one_batch(\n","  model: nn.Module,\n","  criterion: nn.Module,\n","  optimizer: Optimizer,\n","  inputs: torch.Tensor,\n","  targets: torch.Tensor,\n","  scheduler\n",") -> None:\n","\n","    # TODO:\n","    # Step 1: Reset the gradients (from the last iteration)\n","\n","    # Step 2: Forward inference\n","\n","    # Step 3: Calculate the loss\n","\n","    # Step 4: Backward propagation\n","\n","    # Step 5: Update optimizer\n","\n","    # (Optional Step 6: scheduler)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3kePTCanalUE"},"source":["We then define the training function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SWx96SGajDR"},"outputs":[],"source":["def train(\n","    model: nn.Module,\n","    dataflow: DataLoader,\n","    criterion: nn.Module,\n","    optimizer: Optimizer,\n","    # scheduler: LRScheduler\n","):\n","\n","  model.train()\n","\n","  for inputs, targets in tqdm(dataflow, desc='train', leave=False):\n","    # Move the data from CPU to GPU\n","    inputs = inputs.cuda()\n","    targets = targets.cuda()\n","\n","    # Call train_one_batch function\n","    train_one_batch(model, criterion, optimizer, inputs, targets)"]},{"cell_type":"markdown","metadata":{"id":"QGaYWFFCbD32"},"source":["Last, we define the evaluation function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXVXHqimbOIo"},"outputs":[],"source":["def evaluate(\n","  model: nn.Module,\n","  dataflow: DataLoader\n",") -> float:\n","\n","    model.eval()\n","    num_samples = 0\n","    num_correct = 0\n","\n","    with torch.no_grad():\n","        for inputs, targets in tqdm(dataflow, desc=\"eval\", leave=False):\n","            # TODO:\n","            # Step 1: Move the data from CPU to GPU\n","\n","            # Step 2: Forward inference\n","\n","            # Step 3: Convert logits to class indices (predicted class)\n","\n","            # Update metrics\n","            num_samples += targets.size(0)\n","            num_correct += (predicts == targets).sum()\n","\n","    return (num_correct / num_samples * 100).item()"]},{"cell_type":"markdown","metadata":{"id":"HWCOYuj5cNg3"},"source":["With training and evaluation functions, we can finally start training the model!\n","\n","If the training is done properly, the accuracy should simply reach higher than 0.925:\n","\n","***Please screenshot the output model accuracy, hand in as YourID_acc_1.png***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"czZObK4OcPD-"},"outputs":[],"source":["for epoch_num in tqdm(range(1, NUM_EPOCH + 1)):\n","  train(model, dataflow[\"train\"], criterion, optimizer, scheduler)\n","  acc = evaluate(model, dataflow[\"test\"])\n","  print(f\"epoch {epoch_num}:\", acc)\n","\n","print(f\"final accuracy: {acc}\")"]},{"cell_type":"markdown","metadata":{"id":"Ao7C2ZSudE0o"},"source":["Save the weight of the model as \"model.pt\":"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vQrW_B-bcygR"},"outputs":[],"source":["# TODO:\n","# Save the model weight\n","\n"]},{"cell_type":"markdown","metadata":{"id":"r6pbapVjdTG3"},"source":["You will find \"model.pt\" in the current folder."]},{"cell_type":"markdown","metadata":{"id":"TqwrM470RNoO"},"source":["### Export Model (5%)"]},{"cell_type":"markdown","metadata":{"id":"BHpw1pWF6d1c"},"source":["We can also save the model weight in [ONNX Format](https://pytorch.org/docs/stable/onnx_torchscript.html):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yd8oq4-O6kla"},"outputs":[],"source":["import torch.onnx\n","\n","# TODO:\n","# Specify the input shape\n","\n","onnx_path = 'model.onnx'\n","\n","# TODO:\n","# Export the model to ONNX format\n","\n","print(f\"Model exported to {onnx_path}\")"]},{"cell_type":"markdown","metadata":{"id":"mLnLCIuv7M46"},"source":["In onnx format, we can observe the model structure using [Netron](https://netron.app/).\n","\n","***Please download the model structure, hand in as YourID_onnx.png.***"]},{"cell_type":"markdown","metadata":{"id":"0QjgTo0GduJx"},"source":["### Inference (10%)"]},{"cell_type":"markdown","metadata":{"id":"pEH0zddHdzH-"},"source":["Load the saved model weight:\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i28yt-XOdweL"},"outputs":[],"source":["# TODO:\n","# Step 1: Get the model structure (mobilenet_v2 and the classifier)\n","loaded_model =\n","\n","# Step 2: Load the model weight from \"model.pt\".\n","\n","# Step 3: Send the model from cpu to gpu\n"]},{"cell_type":"markdown","metadata":{"id":"IEaZWcuheRFv"},"source":["Run inference with the loaded model weight and check the accuracy\n","\n","***Please screenshot the output model accuracy, hand in as YourID_acc_2.png***"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2dMRsPgGeYe6"},"outputs":[],"source":["acc = evaluate(loaded_model, dataflow[\"test\"])\n","print(f\"accuracy: {acc}\")"]},{"cell_type":"markdown","metadata":{"id":"ytdLy6EIfoeT"},"source":["If the accurracy is the same as the accuracy before saved, you have completed PART 1.\n","\n","Congratulations!"]},{"cell_type":"markdown","metadata":{"id":"ni0FnXXHcgXa"},"source":["# **Part 2: LLM with torch.compile**"]},{"cell_type":"markdown","metadata":{"id":"QiOzZkitfkmM"},"source":["In part 2, we will compare the inference speed of the LLM whether we use torch.compile.\n","\n","```torch.compile``` is a new feature in PyTorch 2.0.\n","\n","The following tutorial will help you get to know the usage.\n","\n","[Introduction to torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html)\n","\n","We will choose ```Llama-3.2-1B-Instruct``` as our LLM model.\n","\n","Make sure you have access to llama before starting Part 2.\n","\n","https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct"]},{"cell_type":"markdown","metadata":{"id":"bu-ihO_RiaVM"},"source":["### Loading LLM (20%)"]},{"cell_type":"markdown","metadata":{"id":"p3LSb5aoiodV"},"source":["We will first install huggingface and login with your token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7EhC4kl0_CUf","collapsed":true},"outputs":[],"source":["!pip install -U \"huggingface_hub[cli]\"\n","!huggingface-cli login"]},{"cell_type":"markdown","metadata":{"id":"mUD-npfwki3T"},"source":["We choose LLaMa 3.2 1B Instruct as our LLM model and load the pretrained model.\n","\n","Model ID: **\"meta-llama/Llama-3.2-1B-Instruct\"**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SjCkbW_i8VUq"},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","\n","# TODO:\n","# Load the LLaMA 3.2 1B Instruct model\n","model_id =\n","tokenizer = AutoTokenizer.from_pretrained()\n","model = AutoModelForCausalLM.from_pretrained(, torch_dtype=torch.float16).cuda()"]},{"cell_type":"markdown","metadata":{"id":"m1F8VBtODHX2"},"source":["First we need to decide our prompt to feed into LLM and the maximum token length as well.\n","\n","You can also change the iteration times of testing for the following tests."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJMPtSLuCzCq"},"outputs":[],"source":["# TODO:\n","# Input prompt\n","# You can change the prompt whatever you want, e.g. \"How to learn a new language?\", \"What is Edge AI?\"\n","\n","prompt =\n","inputs = tokenizer(, return_tensors=\"pt\").to(\"cuda\")\n","max_token_length =\n","iter_times = 10"]},{"cell_type":"markdown","source":["### Inference with torch.compile (10%)\n"],"metadata":{"id":"RiHloHdRKaaY"}},{"cell_type":"markdown","source":["Let's define a timer function to compare the speed up of ```torch.compile```"],"metadata":{"id":"ZWj3sD_PJL7Z"}},{"cell_type":"code","source":["def timed(fn):\n","  start = torch.cuda.Event(enable_timing=True)\n","  end = torch.cuda.Event(enable_timing=True)\n","  start.record()\n","  result = fn()\n","  end.record()\n","  torch.cuda.synchronize()\n","  return result, start.elapsed_time(end) / 1000"],"metadata":{"id":"oNK4ukXZb631"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After everything is set up, let's start!\n","\n","We first simply run the inference without ```torch.compile```\n"],"metadata":{"id":"k4IDTZYbJxJj"}},{"cell_type":"code","source":["original_times = []\n","\n","# Timing without torch.compile\n","for i in range(iter_times):\n","  with torch.no_grad():\n","    original_output, original_time = timed(lambda: model.generate(**inputs, max_length=max_token_length, pad_token_id=tokenizer.eos_token_id))\n","  original_times.append(original_time)\n","  print(f\"Time taken without torch.compile: {original_time} seconds\")\n","\n","# Decode the output\n","output_text = tokenizer.decode(original_output[0], skip_special_tokens=True)\n","print(f\"Output without torch.compile: {output_text}\")"],"metadata":{"id":"-u-fBCU-b7hS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Before using ```torch.compile```, we need to access the model's ```generation_config``` attribute and set the ```cache_implementation``` to \"static\".\n","\n","To use ```torch.compile```, we need to call ```torch.compile``` on the model to compile the forward pass with the static kv-cache.\n","\n","Reference: https://huggingface.co/docs/transformers/llm_optims?static-kv=basic+usage%3A+generation_config"],"metadata":{"id":"EeVhUlk_K9La"}},{"cell_type":"code","source":["compile_times = []\n","\n","# Remind that whenever you use torch.compile, you need to use torch._dynamo.reset() to clear all compilation caches and restores the system to its initial state.\n","import torch._dynamo\n","torch._dynamo.reset()\n","\n","# TODO:\n","# Compile the model\n","model.generation_config.cache_implementation = \"static\"\n","compiled_model =\n","\n","# Timing with torch.compile\n","for i in range(iter_times):\n","  with torch.no_grad():\n","    compile_output, compile_time = timed(lambda: compiled_model.generate(**inputs, max_length=max_token_length, pad_token_id=tokenizer.eos_token_id))\n","  compile_times.append(compile_time)\n","  print(f\"Time taken with torch.compile: {compile_time} seconds\")\n","\n","# Decode output\n","output_text = tokenizer.decode(compile_output[0], skip_special_tokens=True)\n","print(f\"\\nOutput with torch.compile: {output_text}\")"],"metadata":{"id":"BQkTIDusb_7i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can easily observe that after the first inference, the inference time drops a lot!\n","\n","Below code can tell you how much faster did ```torch.compile``` did.\n","\n","***Please screenshot the inference time and speedup below, hand in as YourID_speedup.png***"],"metadata":{"id":"NFZMBADDTo5T"}},{"cell_type":"code","source":["import numpy as np\n","original_med = np.median(original_times)\n","compile_med = np.median(compile_times)\n","speedup = original_med / compile_med\n","print(f\"Original median: {original_med},\\nCompile median: {compile_med},\\nSpeedup: {speedup}x\")"],"metadata":{"id":"gPbqAgo6P7et"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pXdb901VmrZS"},"source":["You've finished part 2.\n","\n","Congratulations!"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}